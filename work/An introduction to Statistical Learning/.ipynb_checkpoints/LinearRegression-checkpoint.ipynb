{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5cfa170-2bbd-41e7-b816-51af8177863a",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a7e06-2bac-4703-aa0e-267d4720f6b1",
   "metadata": {},
   "source": [
    "## Statistical Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48400b3-8841-4e20-a4b5-c040f257559c",
   "metadata": {},
   "source": [
    "Suppose we observe a quantitative response **Y** and *p* different predictors **X**, $X=X_{1},X_{2},X_{3},\\dots,X_{p}$. We assume that there's a relationship between Y and X such that\n",
    "$Y = f(X)+\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf51073-b854-458d-8fbc-8c24ea4052cc",
   "metadata": {},
   "source": [
    "Consider a given estimate $\\hat{f}$ and a set of predictors **X**, which yields the prediction $\\hat{Y}= \\hat{f}(X)$. Assume for a moment that both $\\hat{f}$ and $X$ are fixed. Then, it is easy to show that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc94d90a-fd63-4d10-b8b1-34b8aaf635d6",
   "metadata": {},
   "source": [
    "$$\n",
    "E(Y-\\hat{Y})^2 = E[ f(X) + \\epsilon - \\hat{f}(X)]^2 = [ f(X) - \\hat{f}(X)]^2 + Var(\\epsilon)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852c0be-5d8f-4cc5-82fa-08cd4ffae949",
   "metadata": {},
   "source": [
    "where $E(Y-\\hat{Y})^2$ represents the average, or the expected value, of the squared difference between the predicted and the actual value of Y, and $Var(\\epsilon)$ represents the variance associated with the error term $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7041840-df2b-49c9-8013-2da1aa8134c1",
   "metadata": {},
   "source": [
    "### Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b9f312-c498-4ce7-8d6e-07cecf25e629",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{f}(x_i))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97791ee-7fc1-4ad0-988e-a69c2661a466",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef9e62-8ae9-43db-b68b-36653a253bf0",
   "metadata": {},
   "source": [
    "$Y \\approx \\beta_0+\\beta_1 X$ where $\\beta_0$ is the intercept and $\\beta_1$ is the slope and together are known as coeffiecients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6415afe2-5908-4ce9-9335-87df95f4453e",
   "metadata": {},
   "source": [
    "As such, once a model is trained, we can predict:\n",
    "$$\n",
    "\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x\n",
    "$$\n",
    "\n",
    "where $\\hat{y}$ indicates a prediction of $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3e72b-e249-476c-8fe9-f1b28220347e",
   "metadata": {},
   "source": [
    "Let $\\hat{y}_i = \\hat{\\beta_0}+\\hat{\\beta_1}x_i$ be the prediction for Y based on the ith value ox X, then $\\epsilon_i=y_i-\\hat{y}_i$ represents the ith residual. Then, \n",
    "we can define the residual sum of squares (RSS) as:\n",
    "$$\n",
    "RSS = \\epsilon_{0}^2+\\epsilon_{1}^2+\\dots+\\epsilon_{n}^2\n",
    "$$\n",
    "and the residual standard error (RSE) as:\n",
    "\n",
    "$$\n",
    "RSE = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae6067-19f4-47aa-a762-b7c2dd8b3f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#page 110"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
